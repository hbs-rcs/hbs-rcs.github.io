<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stata | HBS Research Computing Services Blog</title>
    <link>https://hbs-rcs.github.io/tag/stata/</link>
      <atom:link href="https://hbs-rcs.github.io/tag/stata/index.xml" rel="self" type="application/rss+xml" />
    <description>Stata</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 10 Oct 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hbs-rcs.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Stata</title>
      <link>https://hbs-rcs.github.io/tag/stata/</link>
    </image>
    
    <item>
      <title>Specification Curve Analysis: Overview and Stata Example</title>
      <link>https://hbs-rcs.github.io/post/specification-curve-analysis/</link>
      <pubDate>Sun, 10 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://hbs-rcs.github.io/post/specification-curve-analysis/</guid>
      <description>

&lt;div id=&#34;overview&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Overview&lt;/h2&gt;
&lt;p&gt;Research reproducibility topic has been gaining momentum over the past decade. There have been many studies reporting inability to replicate published results and lack of necessary details in methods description. Some journals are addressing this issue by requiring access to study data and executable code. However, while this may provide some reassurance in reliability of the results, the actual choice of analytic methods could be shaped by many assumptions that might not be evident.&lt;/p&gt;
&lt;p&gt;There usually isn’t one correct way to analyse data. Instead, empirical studies often have plenty of flexibility in the way data are collected and cleaned as well as in the final model specification. A data cleaning step may involve exclusion of some units with missing data or conversion of a continuous variable to a categorical one or (vice versa). There also might be models equally plausible for the outcome, but having different sets of covariates or functional forms. Each of these small steps may snowball into a reported effect that is overly favorable to researchers’ narrative.&lt;/p&gt;
&lt;p&gt;A relatively novel and very promising method that can help to mitigate this issue was proposed in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-simonsohn2015better&#34; role=&#34;doc-biblioref&#34;&gt;Simonsohn, Simmons, and Nelson&lt;/a&gt; (&lt;a href=&#34;#ref-simonsohn2015better&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; and is called &lt;strong&gt;Specification Curve Analysis&lt;/strong&gt; (SCA). The idea behind the method is simple - the researcher is asked to consider multiple plausible ways to analyze the data and show that, &lt;em&gt;jointly&lt;/em&gt;, the null hypothesis of no effect can be rejected. It doesn’t mean that all models must result in a statistically significant effect (though, it would make the conclusions very convincing!). However, even if the effect is detected when all specifications are tested simultaneously, this would result in a more objective inference.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;method-details&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Method Details&lt;/h2&gt;
&lt;p&gt;The method involves the following steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: lower-roman&#34;&gt;
&lt;li&gt;identifying the set of theoretically justified, statistically valid, and non-redundant analytic specifications;&lt;/li&gt;
&lt;li&gt;running the analysis for each specification and displaying the results graphically - this allows the readers to identify consequential specification decisions;&lt;/li&gt;
&lt;li&gt;conducting statistical tests to determine whether, as a whole, results are inconsistent with the null hypothesis.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first two steps above are self-explanatory. However, the third step is novel. The authors (&lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-simonsohn2020specification&#34; role=&#34;doc-biblioref&#34;&gt;Simonsohn, Simmons, and Nelson&lt;/a&gt; (&lt;a href=&#34;#ref-simonsohn2020specification&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;) proposed three test statistics for the SCA:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;median effect estimated across all specifications;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;share of specifications that obtain a statistically significant effect in the predicted direction;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;average of Z-values across all specifications.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For each of them a sampling distribution can be generating by “resampling under-the-null.” This involves modifying the observed data so that the null hypothesis is known to be true, and then drawing random samples of the modified data. The test statistic of interest is then computed on each of those samples. The resulting distribution is the estimated distribution of the test statistic under the null.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;available-tools&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Available Tools&lt;/h2&gt;
&lt;p&gt;There are several resources available to aid the implementation of the method. I organize them in a table below:&lt;/p&gt;
&lt;table&gt;
&lt;colgroup&gt;
&lt;col width=&#34;2%&#34; /&gt;
&lt;col width=&#34;15%&#34; /&gt;
&lt;col width=&#34;82%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Language&lt;/th&gt;
&lt;th&gt;Package Name&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://masurp.github.io/specr/&#34;&gt;specr&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Available on CRAN. Provides functions to set up, run, evaluate and plot the specifications of interest. There is a lot of flexibility in model set-up. However, the package doesn’t have capability to perform the step (iii) above (i.e., the joint testing).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;R&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://joachim-gassen.github.io/rdfanalysis/&#34;&gt;rdfanalysis&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Available only on GitHub. A more comprehensive collection of functions that provides a self-documenting code base that allows researchers to systematically document and explore their researcher degrees of freedom when conducting analyses. Has a shiny front end that helps to explore the findings interactively.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Stata&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/martin-andresen/speccurve&#34;&gt;speccurve&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;One function that can only plot the curve using coefficients stored in the &lt;strong&gt;e()&lt;/strong&gt;-returns. Requires setting up and looping through the models beforehand.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Stata&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/mgao6767/specurve&#34;&gt;specurve&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Depends on Stata 16’s Python (v.3.6) integration and several additional Python modules. The function performs regressions as specified in a provided YAML-formatted file and plots the specification curve. Limited to &lt;code&gt;reghdfe&lt;/code&gt; models only, but allows for various combinations of fixed effects and clustering.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;Stata&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/bbdaniels/specc&#34;&gt;specc&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Available on &lt;a href=&#34;https://econpapers.repec.org/software/bocbocode/s458720.htm&#34;&gt;SSC&lt;/a&gt; and is open for development on GitHub. The package appears to be very flexible in setting up models and enumerating specifications as well as plotting the curve. However, it lacks a simple example to get started.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;Python&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;https://github.com/aeturrell/specification_curve&#34;&gt;specification_curve&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Allows to conduct analysis and plot specification curves. Flexible in model specification and very well &lt;a href=&#34;https://specification-curve.readthedocs.io/en/latest/&#34;&gt;documented&lt;/a&gt;. While it also can’t perform the joint test (step (iii) of the specification analysis), the author has an example of its manual implementation &lt;a href=&#34;http://aeturrell.com/2019/01/25/Specification-Curve-Analysis/&#34;&gt;here&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It looks like most major statistical programming language have some version of the specification curve implemented. However, as far as I can tell, none of them are capable of performing step (iii), which, arguably, is as important as the curve itself. Therefore, for now, researches have to implement it themselves or contact RCS (research@hbs.edu) for assistance!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stata-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stata Example&lt;/h2&gt;
&lt;p&gt;Next, I show an example in Stata that loops through several model specifications and then uses the &lt;a href=&#34;https://github.com/martin-andresen/speccurve&#34;&gt;speccurve&lt;/a&gt; function in Stata to plot the curve. Before running this code, make sure that the function is installed in Stata by running the following line:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;net install speccurve, from(&#34;https://raw.githubusercontent.com/martin-andresen/speccurve/master&#34;)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;The code uses a classic &lt;code&gt;auto&lt;/code&gt; data set and specifies several regression models that predict car price using available characteristics. The effect of interest is the coefficient estimated for the indicator &lt;code&gt;foreign&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;stata&#34;&gt;&lt;code&gt;clear all
sysuse auto, clear

loc no=0

* enumerationg many different specifications using a loop
foreach m in &amp;quot;&amp;quot; &amp;quot;mpg&amp;quot; {
    foreach tr in &amp;quot;&amp;quot; &amp;quot;trunk&amp;quot; {
        foreach wt in &amp;quot;&amp;quot; &amp;quot;weight&amp;quot; {
            foreach ln in &amp;quot;&amp;quot; &amp;quot;length&amp;quot; {
                foreach hr in &amp;quot;&amp;quot; &amp;quot;headroom&amp;quot; {
                    qui reg price foreign `m&amp;#39; `tr&amp;#39; `wt&amp;#39; `ln&amp;#39; `hr&amp;#39;
                  eststo md`no&amp;#39;
                    loc ++no
                }
            }
        }
    }
}

* plotting a SC with foreign as a parameter of interest
speccurve *, param(foreign) controls title(SCA for the effect of foreigh)
graph export &amp;quot;speccurve1.svg&amp;quot;, replace&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(1978 Automobile Data)




(file speccurve1.svg written in SVG format)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code above produced the following specification curve:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;speccurve1.svg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks like including the &lt;code&gt;weight&lt;/code&gt; variable in the model had a notable effect on the coefficient for &lt;code&gt;foreign&lt;/code&gt;. Function &lt;code&gt;speccurve&lt;/code&gt; is somewhat limited in that it doesn’t work with models that have factors as controls. Next, I show a workaround for the latter case:&lt;/p&gt;
&lt;pre class=&#34;stata&#34;&gt;&lt;code&gt;clear all
sysuse auto, clear

egen headroom_c = group(headroom)
loc no=0

foreach m in &amp;quot;&amp;quot; &amp;quot;mpg&amp;quot; {
    foreach tr in &amp;quot;&amp;quot; &amp;quot;trunk&amp;quot; {
        foreach wt in &amp;quot;&amp;quot; &amp;quot;weight&amp;quot; {
            foreach ln in &amp;quot;&amp;quot; &amp;quot;length&amp;quot; {
                foreach hr in &amp;quot;&amp;quot; &amp;quot;headroom&amp;quot; &amp;quot;i.headroom_c&amp;quot;{
                    qui reg price foreign `m&amp;#39; `tr&amp;#39; `wt&amp;#39; `ln&amp;#39; `hr&amp;#39;
                    
                    qui estadd scalar mpgv = 0, replace
                    qui estadd scalar trunkv = 0, replace
                    qui estadd scalar weightv = 0, replace
                    qui estadd scalar lengthv = 0, replace
                    foreach vr in m tr wt ln {
                        if &amp;quot;``vr&amp;#39;&amp;#39;&amp;quot;!=&amp;quot;&amp;quot; qui estadd scalar ``vr&amp;#39;&amp;#39;v = 1, replace
                    }
                    
                    qui estadd scalar headroomv = 0
                    qui estadd scalar iheadroom_cv = 0
                    local vname = subinstr(&amp;quot;`hr&amp;#39;&amp;quot;, &amp;quot;.&amp;quot;, &amp;quot;&amp;quot;, .)
                    qui estadd scalar `vname&amp;#39;v = 1, replace 
                    
                    eststo md`no&amp;#39;
                    loc ++no
                }
            }
        }
    }
}

* The code below produces an error:
*speccurve *, param(foreign) controls title(SCA for the effect of foreigh)

* Workaround:
speccurve *, param(foreign) level(95) graphopts(legend(pos(1))) title(SCA for auto dataset) panel(mpgv trunkv weightv lengthv headroomv iheadroom_cv)
graph export &amp;quot;speccurve2.svg&amp;quot;, replace&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(1978 Automobile Data)





(file speccurve2.svg written in SVG format)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The code implements models that have &lt;code&gt;headroom&lt;/code&gt; included as a factor or as a continuous variable. Note that the first call for &lt;code&gt;speccurve&lt;/code&gt; would produce an error due to a bug in the function. However, the second call produces the following specification curve:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;speccurve2.svg&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One can also output a table with numerical results:&lt;/p&gt;
&lt;pre class=&#34;stata&#34;&gt;&lt;code&gt;matlist r(table)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;             |    specno    modelno   estimate      min95      max95       mpgv     trunkv    weightv    lengthv  headroomv  iheadro~v 
-------------+-------------------------------------------------------------------------------------------------------------------------
         md0 |         1          1   312.2587  -1191.708   1816.225          0          0          0          0          0          0 
         md2 |         2          3    364.925  -1419.362   2149.212          0          0          0          0          0          1 
         md1 |         3          2   577.8125  -992.5493   2148.174          0          0          0          0          1          0 
        md14 |         4         15   740.7716  -960.3329   2441.876          0          1          0          0          0          1 
        md13 |         5         14   1128.818  -393.3118   2650.948          0          1          0          0          1          0 
        md12 |         6         13   1190.155  -326.8468   2707.157          0          1          0          0          0          0 
        md26 |         7         27   1327.396  -294.4929   2949.285          1          0          0          0          0          1 
        md38 |         8         39   1376.011  -230.2271   2982.249          1          1          0          0          0          1 
        md25 |         9         26   1714.109   292.4855   3135.733          1          0          0          0          1          0 
        md24 |        10         25   1767.292   371.2169   3163.368          1          0          0          0          0          0 
        md37 |        11         38   1825.733   408.1118   3243.355          1          1          0          0          1          0 
        md36 |        12         37   1887.461   468.5866   3306.335          1          1          0          0          0          0 
        md41 |        13         42   2196.194   517.1768   3875.212          1          1          0          1          0          1 
        md29 |        14         30   2247.635   591.1235   3904.146          1          0          0          1          0          1 
        md17 |        15         18   2294.095   616.0623   3972.129          0          1          0          1          0          1 
         md5 |        16          6   2352.064   696.5941   4007.534          0          0          0          1          0          1 
        md40 |        17         41   2615.666   1084.272   4147.059          1          1          0          1          1          0 
        md27 |        18         28   2644.771   1125.227   4164.315          1          0          0          1          0          0 
        md28 |        19         29   2644.847   1133.077   4156.616          1          0          0          1          1          0 
        md39 |        20         40   2670.519   1133.691   4207.347          1          1          0          1          0          0 
        md16 |        21         17   2774.021   1233.682   4314.361          0          1          0          1          1          0 
         md3 |        22          4   2801.143   1273.549   4328.737          0          0          0          1          0          0 
         md4 |        23          5   2801.899   1281.258    4322.54          0          0          0          1          1          0 
        md15 |        24         16   2827.236    1282.39   4372.082          0          1          0          1          0          0 
        md23 |        25         24   3072.365   1665.236   4479.495          0          1          1          1          0          1 
        md47 |        26         48   3079.179   1643.422   4514.937          1          1          1          1          0          1 
        md20 |        27         21   3116.728   1652.392   4581.064          0          1          1          0          0          1 
         md8 |        28          9   3132.815    1688.75    4576.88          0          0          1          0          0          1 
        md11 |        29         12   3146.808   1750.304   4543.312          0          0          1          1          0          1 
        md35 |        30         36   3148.211   1721.962   4574.459          1          0          1          1          0          1 
        md44 |        31         45   3162.517   1671.851   4653.184          1          1          1          0          0          1 
        md32 |        32         33   3179.193   1706.637   4651.749          1          0          1          0          0          1 
        md46 |        33         47   3502.516   2193.975   4811.056          1          1          1          1          1          0 
        md22 |        34         23    3526.83   2250.662   4802.999          0          1          1          1          1          0 
        md34 |        35         35   3545.345   2248.433   4842.256          1          0          1          1          1          0 
        md33 |        36         34   3550.194   2242.594   4857.793          1          0          1          1          0          0 
        md45 |        37         46   3557.085     2235.3   4878.871          1          1          1          1          0          0 
        md10 |        38         11   3570.379   2305.781   4834.976          0          0          1          1          1          0 
         md9 |        39         10   3573.092   2297.992   4848.191          0          0          1          1          0          0 
        md21 |        40         22   3580.051   2290.845   4869.256          0          1          1          1          0          0 
         md7 |        41          8    3623.75   2316.374   4931.127          0          0          1          0          1          0 
        md19 |        42         20   3631.585    2310.07   4953.101          0          1          1          0          1          0 
         md6 |        43          7   3637.001   2303.885   4970.118          0          0          1          0          0          0 
        md31 |        44         32   3648.619   2310.079   4987.159          1          0          1          0          1          0 
        md43 |        45         44   3654.777   2302.875   5006.679          1          1          1          0          1          0 
        md30 |        46         31    3673.06   2308.909   5037.212          1          0          1          0          0          0 
        md18 |        47         19   3686.447   2352.692   5020.201          0          1          1          0          0          0 
        md42 |        48         43   3711.123   2346.938   5075.308          1          1          1          0          0          0 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-simonsohn2015better&#34; class=&#34;csl-entry&#34;&gt;
Simonsohn, Uri, Joseph P Simmons, and Leif D Nelson. 2015. &lt;span&gt;“Better p-Curves: Making p-Curve Analysis More Robust to Errors, Fraud, and Ambitious p-Hacking, a Reply to Ulrich and Miller (2015).”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-simonsohn2020specification&#34; class=&#34;csl-entry&#34;&gt;
———. 2020. &lt;span&gt;“Specification Curve Analysis.”&lt;/span&gt; &lt;em&gt;Nature Human Behaviour&lt;/em&gt; 4 (11): 1208–14.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Interpreting interaction term in a regression model</title>
      <link>https://hbs-rcs.github.io/post/2017-02-16-interpret_interaction/</link>
      <pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://hbs-rcs.github.io/post/2017-02-16-interpret_interaction/</guid>
      <description>


&lt;div id=&#34;interaction-with-two-binary-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interaction with two binary variables&lt;/h1&gt;
&lt;p&gt;In a regression model with interaction term, people tend to pay attention to only the coefficient of the interaction term.&lt;/p&gt;
&lt;p&gt;Let’s start with the simpliest situation: &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; are binary and coded 0/1.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E(y) = \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1x_2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, we have a saturated model; that is, we have three coefficients representing additive effects from the baseline situation (both &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; being 0). There are four different situations, with four combinations of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A lot of people just pay attention to the interaction term. In the case of studying treatment effects between two groups, say female and male, that makes sense, the interaction term representing the difference between male and female in terms of treatment effect.&lt;/p&gt;
&lt;p&gt;In this model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E(y) = \beta_1 female + \beta_2 treatment + \beta_{12} female*treatment \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The two dummy-coded binary variables, female and treatment, form four combinations. The following 2x2 table represents the expected means of the four cells(combinations).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;male&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;female&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;control&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_1\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;treatment&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_2\)&lt;/span&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_1 + \beta_2 + \beta_{12}\)&lt;/span&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can see from this table that, for example,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_0=E(Y|(0,0))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is, &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt; is the expected mean of the cell (0,0) (male and control).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\beta_0 + \beta_1 =E(Y|(1,0))\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is ,&lt;span class=&#34;math inline&#34;&gt;\(\beta_0 + \beta_1\)&lt;/span&gt; is the expected mean of the cell (1,0) (female and control). And so on.&lt;/p&gt;
&lt;p&gt;Now,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \beta_{12} = (E(Y|(1,1))-E(Y|(0,1)))-(E(Y|(1,0))-E(Y|(0,0))) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;that is, the coefficient on the interaction term is actually the difference in difference. That’s why in many situations, people are only interested in the interaction coefficient, since they are only interested in the diff-in-diff estimates. The usually diff-in-diff estimator in causal inference literature refer to something similar, instead of female vs. male, people are interested in the treatment effect difference in before and after treatment. If we simply replace female/male dummy with before/after dummy, we can use the same logic. In those situations, it’s fine to mainly focus on the interaction term coefficient.&lt;/p&gt;
&lt;p&gt;In some other situations, the three coefficients are equally important. It depends on your interest. For example, if we are interested in studying differences between union member and non-union member and black vs. non-black, we may not be only interested in the interaction effect. Instead, we might be interested in all four cells, maybe all possible pairwise comparisons. In that case, we should pay attention to all three coefficients. Stata’s “margins” command is of great help if we’d like to compare the cell means.&lt;/p&gt;
&lt;p&gt;Let’s take a look from a sample example in Stata:&lt;/p&gt;
&lt;pre class=&#34;stata&#34;&gt;&lt;code&gt;webuse union3
reg ln_wage i.union##i.black, r
margins union#black
margins union#black, pwcompare
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## . webuse union3
## (National Longitudinal Survey.  Young Women 14-26 years of age in 1968)
## 
## . reg ln_wage i.union##i.black, r
## 
## Linear regression                               Number of obs     =      1,244
##                                                 F(3, 1240)        =      34.76
##                                                 Prob &amp;gt; F          =     0.0000
##                                                 R-squared         =     0.0762
##                                                 Root MSE          =     .37699
## 
## ------------------------------------------------------------------------------
##              |               Robust
##      ln_wage |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##      1.union |   .2045053   .0291682     7.01   0.000     .1472808    .2617298
##      1.black |  -.1709034   .0308067    -5.55   0.000    -.2313425   -.1104644
##              |
##  union#black |
##         1 1  |   .0386275   .0516609     0.75   0.455     -.062725      .13998
##              |
##        _cons |   1.657525   .0138278   119.87   0.000     1.630396    1.684653
## ------------------------------------------------------------------------------
## 
## . margins union#black
## 
## Adjusted predictions                            Number of obs     =      1,244
## Model VCE    : Robust
## 
## Expression   : Linear prediction, predict()
## 
## ------------------------------------------------------------------------------
##              |            Delta-method
##              |     Margin   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##  union#black |
##         0 0  |   1.657525   .0138278   119.87   0.000     1.630396    1.684653
##         0 1  |   1.486621    .027529    54.00   0.000     1.432613     1.54063
##         1 0  |    1.86203   .0256822    72.50   0.000     1.811644    1.912415
##         1 1  |   1.729754   .0325611    53.12   0.000     1.665873    1.793635
## ------------------------------------------------------------------------------
## 
## . margins union#black, pwcompare
## 
## Pairwise comparisons of adjusted predictions
## Model VCE    : Robust
## 
## Expression   : Linear prediction, predict()
## 
## -----------------------------------------------------------------
##                 |            Delta-method         Unadjusted
##                 |   Contrast   Std. Err.     [95% Conf. Interval]
## ----------------+------------------------------------------------
##     union#black |
## (0 1) vs (0 0)  |  -.1709034   .0308067     -.2313425   -.1104644
## (1 0) vs (0 0)  |   .2045053   .0291682      .1472808    .2617298
## (1 1) vs (0 0)  |   .0722294   .0353756      .0028268     .141632
## (1 0) vs (0 1)  |   .3754087   .0376487      .3015466    .4492709
## (1 1) vs (0 1)  |   .2431328   .0426388      .1594807     .326785
## (1 1) vs (1 0)  |  -.1322759   .0414705     -.2136359   -.0509159
## -----------------------------------------------------------------
## 
## .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What we get by using “margins union#black” is the four cell means of &lt;span class=&#34;math inline&#34;&gt;\(E(Y)\)&lt;/span&gt;, in this case, log of wage. Then “margins union#black, pwcompare” tells us all pairwise comparison of these four cell means. Instead of only paying attention to the interaction coefficient, in this case we might be interested in some comparisons of the four different situations of union and black. In fact, in this example, despite the interaction term being insignificant, all six comparisons of the cell means turn out to have 95% confidence intervals that do not include zero.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interaction-with-continuous-variables&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interaction with continuous variables&lt;/h1&gt;
&lt;p&gt;Let’s start with the simpliest situation: &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; are continuous.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ E(y) = \beta_1 x_1 + \beta_2 x_2 + \beta_{12} x_1*x_2 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this case, we recommend “centering” &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; if they are continuous; that is, subtracting the mean value from each continuous independent variable when they are involved in the interaction term. There are two reason for it:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To reduce multi-collinearity. If the range of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; include only positive numbers, then &lt;span class=&#34;math inline&#34;&gt;\(x_1*x_2\)&lt;/span&gt; can be highly correlated with both or one of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. This can lead to numerical problems and unstable coefficient estimates (multi-collinearity problem).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;“Centering” can reduce the correlation between the interaction term and the independent variables. If the original variables are normally distributed, interaction term after centering is actually uncorrelated with the original variables. When they are not normally distributed, centering will still reduce the correlation to a large degree.&lt;/p&gt;
&lt;ol start=&#34;2&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;To help with interpretation. In a model with interaction, &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; represents the effect of &lt;span class=&#34;math inline&#34;&gt;\(x_1\)&lt;/span&gt; when &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; is zero. However, in many situations, zero is not within the range of &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt;. After centering, centered &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; at zero simply means original &lt;span class=&#34;math inline&#34;&gt;\(x_2\)&lt;/span&gt; at its mean value.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When we have dummy variable interacting with continuous variable, only continuous variable should be centered.&lt;/p&gt;
&lt;p&gt;Again, Stata’s margins command is helpful.&lt;/p&gt;
&lt;pre class=&#34;stata&#34;&gt;&lt;code&gt;sysuse auto
sum mpg
gen mpg_centered=mpg-r(mean)
sum mpg_centered
reg price i.foreign##c.mpg_centered
margins foreign, at(mpg_centered=(-3 (1) 3))
marginsplot
graph export marginsplot.eps, replace&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## . sysuse auto
## (1978 Automobile Data)
## 
## . sum mpg
## 
##     Variable |        Obs        Mean    Std. Dev.       Min        Max
## -------------+---------------------------------------------------------
##          mpg |         74     21.2973    5.785503         12         41
## 
## . gen mpg_centered=mpg-r(mean)
## 
## . sum mpg_centered
## 
##     Variable |        Obs        Mean    Std. Dev.       Min        Max
## -------------+---------------------------------------------------------
## mpg_centered |         74   -4.03e-08    5.785503  -9.297297    19.7027
## 
## . reg price i.foreign##c.mpg_centered
## 
##       Source |       SS           df       MS      Number of obs   =        74
## -------------+----------------------------------   F(3, 70)        =      9.48
##        Model |   183435285         3  61145094.9   Prob &amp;gt; F        =    0.0000
##     Residual |   451630112        70  6451858.74   R-squared       =    0.2888
## -------------+----------------------------------   Adj R-squared   =    0.2584
##        Total |   635065396        73  8699525.97   Root MSE        =    2540.1
## 
## ------------------------------------------------------------------------------
##        price |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##      foreign |
##     Foreign  |   1666.519    717.217     2.32   0.023     236.0751    3096.963
## mpg_centered |  -329.2551   74.98545    -4.39   0.000    -478.8088   -179.7013
##              |
##      foreign#|
##           c. |
## mpg_centered |
##     Foreign  |   78.88826   112.4812     0.70   0.485    -145.4485     303.225
##              |
##        _cons |   5588.295   369.0945    15.14   0.000     4852.159    6324.431
## ------------------------------------------------------------------------------
## 
## . margins foreign, at(mpg_centered=(-3 (1) 3))
## 
## Adjusted predictions                            Number of obs     =         74
## Model VCE    : OLS
## 
## Expression   : Linear prediction, predict()
## 
## 1._at        : mpg_centered    =          -3
## 
## 2._at        : mpg_centered    =          -2
## 
## 3._at        : mpg_centered    =          -1
## 
## 4._at        : mpg_centered    =           0
## 
## 5._at        : mpg_centered    =           1
## 
## 6._at        : mpg_centered    =           2
## 
## 7._at        : mpg_centered    =           3
## 
## ------------------------------------------------------------------------------
##              |            Delta-method
##              |     Margin   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##  _at#foreign |
##  1#Domestic  |    6576.06    370.446    17.75   0.000     5837.229    7314.891
##   1#Foreign  |   8005.915   766.8178    10.44   0.000     6476.545    9535.284
##  2#Domestic  |   6246.805   354.4734    17.62   0.000      5539.83     6953.78
##   2#Foreign  |   7755.548   709.9327    10.92   0.000     6339.632    9171.464
##  3#Domestic  |    5917.55   354.0032    16.72   0.000     5211.513    6623.587
##   3#Foreign  |   7505.181   658.8306    11.39   0.000     6191.185    8819.177
##  4#Domestic  |   5588.295   369.0945    15.14   0.000     4852.159    6324.431
##   4#Foreign  |   7254.814   614.9548    11.80   0.000     6028.325    8481.303
##  5#Domestic  |    5259.04    397.981    13.21   0.000     4465.292    6052.788
##   5#Foreign  |   7004.447   579.9479    12.08   0.000     5847.778    8161.117
##  6#Domestic  |   4929.785   437.9413    11.26   0.000     4056.338    5803.231
##   6#Foreign  |   6754.081   555.4891    12.16   0.000     5646.192    7861.969
##  7#Domestic  |    4600.53    486.253     9.46   0.000     3630.729    5570.331
##   7#Foreign  |   6503.714   543.0057    11.98   0.000     5420.723    7586.704
## ------------------------------------------------------------------------------
## 
## . marginsplot
## 
##   Variables that uniquely identify margins: mpg_centered foreign
## 
## . graph export marginsplot.eps, replace
## (note: file marginsplot.eps not found)
## (file marginsplot.eps written in EPS format)&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://hbs-rcs.github.io/post/2017-02-16-interpret_interaction_files/marginsplot.png&#34; /&gt;

&lt;/div&gt;
&lt;p&gt;In this example, the graph shows the predicted price for foreign and domestic cars at different level of mpg.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Marginal effects in models with fixed effects</title>
      <link>https://hbs-rcs.github.io/post/2017-02-16-margins_nonlinear/</link>
      <pubDate>Tue, 14 Feb 2017 00:00:00 +0000</pubDate>
      <guid>https://hbs-rcs.github.io/post/2017-02-16-margins_nonlinear/</guid>
      <description>


&lt;div id=&#34;marginal-effects-in-a-linear-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Marginal effects in a linear model&lt;/h1&gt;
&lt;p&gt;Stata’s margins command has been a powerful tool for many economists. It can calculate predicted means as well as predicted marginal effects. However, we do need to be careful when we use it when fixed effects are included. In a linear model, everything works out fine. However, in a non-linear model, you may not want to use margins, since it’s not calculating what you have in mind.&lt;/p&gt;
&lt;p&gt;In a linear model with fixed effects, we can do it either by “demeaning” every variable, or include dummy variables. They return the same results. Fortunately, marginal effects can be calculated the same way in both models.&lt;/p&gt;
&lt;p&gt;For example:&lt;/p&gt;
&lt;pre class=&#34;stata&#34;&gt;&lt;code&gt;clear
sysuse auto
xtset rep78
xtreg price c.mpg##c.trunk, fe
margins , dydx(mpg)
reg price c.mpg##c.trunk i.rep78
margins , dydx(mpg)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## . clear
## 
## . sysuse auto
## (1978 Automobile Data)
## 
## . xtset rep78
##        panel variable:  rep78 (unbalanced)
## 
## . xtreg price c.mpg##c.trunk, fe
## 
## Fixed-effects (within) regression               Number of obs     =         69
## Group variable: rep78                           Number of groups  =          5
## 
## R-sq:                                           Obs per group:
##      within  = 0.2570                                         min =          2
##      between = 0.0653                                         avg =       13.8
##      overall = 0.2237                                         max =         30
## 
##                                                 F(3,61)           =       7.03
## corr(u_i, Xb)  = -0.4133                        Prob &amp;gt; F          =     0.0004
## 
## ------------------------------------------------------------------------------
##        price |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -98.12003   226.8708    -0.43   0.667    -551.7763    355.5362
##        trunk |   295.0544   343.3934     0.86   0.394    -391.6032     981.712
##              |
##        c.mpg#|
##      c.trunk |  -12.23318   15.94713    -0.77   0.446    -44.12143    19.65506
##              |
##        _cons |    7574.85   5321.325     1.42   0.160    -3065.797     18215.5
## -------------+----------------------------------------------------------------
##      sigma_u |   992.2156
##      sigma_e |  2631.2869
##          rho |  .12449059   (fraction of variance due to u_i)
## ------------------------------------------------------------------------------
## F test that all u_i=0: F(4, 61) = 0.86                       Prob &amp;gt; F = 0.4948
## 
## . margins , dydx(mpg)
## 
## Average marginal effects                        Number of obs     =         69
## Model VCE    : Conventional
## 
## Expression   : Linear prediction, predict()
## dy/dx w.r.t. : mpg
## 
## ------------------------------------------------------------------------------
##              |            Delta-method
##              |      dy/dx   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -268.4981   74.12513    -3.62   0.000    -413.7807   -123.2156
## ------------------------------------------------------------------------------
## 
## . reg price c.mpg##c.trunk i.rep78
## 
##       Source |       SS           df       MS      Number of obs   =        69
## -------------+----------------------------------   F(7, 61)        =      3.19
##        Model |   154453046         7  22064720.8   Prob &amp;gt; F        =    0.0061
##     Residual |   422343913        61  6923670.71   R-squared       =    0.2678
## -------------+----------------------------------   Adj R-squared   =    0.1838
##        Total |   576796959        68  8482308.22   Root MSE        =    2631.3
## 
## ------------------------------------------------------------------------------
##        price |      Coef.   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -98.12003   226.8708    -0.43   0.667    -551.7763    355.5362
##        trunk |   295.0544   343.3934     0.86   0.394    -391.6032     981.712
##              |
##        c.mpg#|
##      c.trunk |  -12.23318   15.94713    -0.77   0.446    -44.12143    19.65506
##              |
##        rep78 |
##           2  |   438.0002   2161.922     0.20   0.840    -3885.031    4761.031
##           3  |   987.1363   2022.606     0.49   0.627    -3057.315    5031.587
##           4  |   1240.944   2046.417     0.61   0.547     -2851.12    5333.008
##           5  |    2605.83   2161.837     1.21   0.233    -1717.031    6928.691
##              |
##        _cons |   6355.731   5209.899     1.22   0.227    -4062.105    16773.57
## ------------------------------------------------------------------------------
## 
## . margins , dydx(mpg)
## 
## Average marginal effects                        Number of obs     =         69
## Model VCE    : OLS
## 
## Expression   : Linear prediction, predict()
## dy/dx w.r.t. : mpg
## 
## ------------------------------------------------------------------------------
##              |            Delta-method
##              |      dy/dx   Std. Err.      t    P&amp;gt;|t|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -268.4981   74.12513    -3.62   0.001    -416.7205   -120.2758
## ------------------------------------------------------------------------------&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All is fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-effects-in-a-non-linear-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Marginal effects in a non-linear model&lt;/h1&gt;
&lt;p&gt;In a nonlinear model, we need to be more careful:&lt;/p&gt;
&lt;pre class=&#34;stata&#34;&gt;&lt;code&gt;clear
sysuse auto
xtset rep78
xtpoisson price mpg trunk, fe
margins , dydx(mpg)
margins , dydx(mpg) predict(nu0)
poisson price mpg trunk i.rep78
margins , dydx(mpg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## . clear
## 
## . sysuse auto
## (1978 Automobile Data)
## 
## . xtset rep78
##        panel variable:  rep78 (unbalanced)
## 
## . xtpoisson price mpg trunk, fe
## 
## Iteration 0:   log likelihood = -39282.052  
## Iteration 1:   log likelihood = -27527.055  
## Iteration 2:   log likelihood = -27518.944  
## Iteration 3:   log likelihood = -27518.944  
## 
## Conditional fixed-effects Poisson regression    Number of obs     =         69
## Group variable: rep78                           Number of groups  =          5
## 
##                                                 Obs per group:
##                                                               min =          2
##                                                               avg =       13.8
##                                                               max =         30
## 
##                                                 Wald chi2(2)      =   22890.68
## Log likelihood  = -27518.944                    Prob &amp;gt; chi2       =     0.0000
## 
## ------------------------------------------------------------------------------
##        price |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -.0450221   .0003814  -118.05   0.000    -.0457696   -.0442746
##        trunk |   .0047349   .0004772     9.92   0.000     .0037996    .0056702
## ------------------------------------------------------------------------------
## 
## . margins , dydx(mpg)
## 
## Average marginal effects                        Number of obs     =         69
## Model VCE    : OIM
## 
## Expression   : Linear prediction, predict()
## dy/dx w.r.t. : mpg
## 
## ------------------------------------------------------------------------------
##              |            Delta-method
##              |      dy/dx   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -.0450221   .0003814  -118.05   0.000    -.0457696   -.0442746
## ------------------------------------------------------------------------------
## 
## . margins , dydx(mpg) predict(nu0)
## 
## Average marginal effects                        Number of obs     =         69
## Model VCE    : OIM
## 
## Expression   : Predicted number of events (assuming u_i=0), predict(nu0)
## dy/dx w.r.t. : mpg
## 
## ------------------------------------------------------------------------------
##              |            Delta-method
##              |      dy/dx   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -.0190939   .0001245  -153.35   0.000    -.0193379   -.0188498
## ------------------------------------------------------------------------------
## 
## . poisson price mpg trunk i.rep78
## 
## Iteration 0:   log likelihood = -27550.942  
## Iteration 1:   log likelihood = -27550.912  
## Iteration 2:   log likelihood = -27550.912  
## 
## Poisson regression                              Number of obs     =         69
##                                                 LR chi2(6)        =   24962.86
##                                                 Prob &amp;gt; chi2       =     0.0000
## Log likelihood = -27550.912                     Pseudo R2         =     0.3118
## 
## ------------------------------------------------------------------------------
##        price |      Coef.   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -.0450221   .0003814  -118.05   0.000    -.0457696   -.0442746
##        trunk |   .0047349   .0004772     9.92   0.000     .0037996    .0056702
##              |
##        rep78 |
##           2  |   .1476657   .0117935    12.52   0.000     .1245509    .1707805
##           3  |   .2295466   .0111741    20.54   0.000     .2076458    .2514474
##           4  |   .2726354   .0112656    24.20   0.000     .2505552    .2947155
##           5  |   .4682657   .0115137    40.67   0.000     .4456992    .4908321
##              |
##        _cons |   9.323117   .0149274   624.57   0.000      9.29386    9.352374
## ------------------------------------------------------------------------------
## 
## . margins , dydx(mpg)
## 
## Average marginal effects                        Number of obs     =         69
## Model VCE    : OIM
## 
## Expression   : Predicted number of events, predict()
## dy/dx w.r.t. : mpg
## 
## ------------------------------------------------------------------------------
##              |            Delta-method
##              |      dy/dx   Std. Err.      z    P&amp;gt;|z|     [95% Conf. Interval]
## -------------+----------------------------------------------------------------
##          mpg |  -276.7079   2.382193  -116.16   0.000    -281.3769   -272.0389
## ------------------------------------------------------------------------------
## 
## .&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this example, “xtpoisson, fe” and “poisson i.rep78” returns the same results. Fixed effect Poisson model (sometimes called conditional fixed effect Poisson) is the same models as a Poisson model with dummies, just like a linear model (OLS with dummies is the same as fixed effect OLS). Poisson model and OLS are unique in this sense that there is no “incidental paramater” problem.&lt;/p&gt;
&lt;p&gt;We see in this example, margins commands do not return the same marginal effects, even though the models are the same. The reason behind this is that in a conditional fixed effect Poisson, the fixed effects are not estimated (they are not in the final likelihood function that gets estimated). Therefore, we’ll have to make a decision what values to use as the values of the fixed effects. “margins, predict(nu0)” simply set all fixed effects to zero. On the other hand, margins after Poisson model with dummies does not do that. The fixed effect in that case gets estimated. Therefore the marginal effects in that case make more sense.&lt;/p&gt;
&lt;p&gt;So our advise for a conditioanl Poisson model is that we should not use margins to calculate marginal effects afterwards; instead, we should simply stick with the original coefficient estimates.&lt;/p&gt;
&lt;p&gt;The same logic applies to the conditional logit model. Fixed effects are not estimated in that model; simply setting them to zero does not make too much sense. In addition, conditional logit model is not the same model as a logit model with dummies, since there is the “incidental paramater” problem. Again, we should just focus on the coefficient estimates as the effect on the logged odds.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
